const contentMD = `\nYou made a machine learning or deep learning model. Amazing! ðŸ¥³\n\nBut how do you check its performance and robustness? Simply building a predictive model is not enough. You have to create a model which gives high accuracy on **out-of-sample data**.\n\nThat's where evaluation metrics come into the picture. They are used to measure the quality of the statistical or machine learning model, and this article will introduce you to the most common and yet important metrics to evaluate your machine learning algorithms.\n\nIf you're someone who just started out in the field of Data Science and Machine Learning, this is for you. We will discuss the following terms:\n\n- Confusion matrix\n- Accuracy\n- Precision\n- Recall\n- F1 score\n- Mean Absolute Error\n- R Squared Score\n- Root Mean Squared Error\n- ROC (Receiver Operating Characteristics) curve\n\nSo letâ€™s dive into it to learn about them :\n\n# Confusion Matrix\n\nA confusion matrix is one of the simplest concepts in the field of machine learning, and yet so important when it comes to statistical classification problems. \n\nA confusion matrix is a N X N matrix, where N is the number of classes being predicted. It is usually used to **describe the performance of a classification model based on the true values known for a data set**.\n\nIn simple words, it is a summary of the prediction result made by the classification model.\n\n### Terms Associated with Confusion Matrix\n\n**True Positive (TP):** True positive means their actual class value is 1 and the predicted value is also 1.\n\nFor example, The case where the woman is actually pregnant and the model also classifies that she is pregnant.\n\n**False Positive (FP):** False positive means their actual class value is 0 and the predicted value is 1.\n\nFor example, The case where the woman is not pregnant and the model classifies that she is pregnant.\n\n**True Negative (TN):** True negative means their actual class value is 0 and the predicted value is also 0.\n\nFor example, The case where the woman is actually not pregnant and the model also classifies that she is not pregnant.\n\n**False Negative (FN):** False-negative means their actual class value is 1 and the predicted value is 0.\n\nFor example, The case where the woman is actually pregnant and the model also classifies that she is not pregnant.\n\n![confusion matrix.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1623853513229/trs6eb65N.png)\n\n> The confusion matrix in itself is not a performance measure but all the performance metrics are dependent on the confusion matrix.\n\n# Accuracy\n\nAccuracy is the most common metric that you can see everywhere when you're evaluating your model. It is simply defined as the number of correct predictions made by your model.\n\nIn simple terms, **accuracy is the ratio of all the correct predictions to the total number of predictions**.\n\nIt can be calculated using a confusion matrix.\n\n![accuracy.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997989964/x6JODZUuU.png)\n\n# Precision\n\nPrecision is defined as **the total number of correctly classified positive examples by the total number of predicted positive examples**.\n\nIn some cases, your classification model might classify based on the most frequent classes. Which in turn will bring a low accuracy because your model didnâ€™t learn anything and just classified based on the top class.\n\nTherefore, we need class-specific performance metrics to analyze. Precision is one of them.\n\n![precision.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997831139/q5VbMq6zx.png)\n\n> It depicts how much the model is right when it says it is right.\n\n\n# Recall\n\nA recall (also known as *sensitivity*) refers to the percentage of total relevant results correctly classified by the classification model.\n\nIt is the **the number of positive samples returned by the custom-trained model**.\n\n\n![Recall.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997843703/IhiNTM2Cn.png)\n\n\nThe relation between recall and precision -\n\n![RecalVsPrecision.jpeg](https://cdn.hashnode.com/res/hashnode/image/upload/v1623900342363/t6_00UhjI.jpeg)\n\n### High recall, low precision:\n\nThis means, most of the positive examples are correctly recognized but there are a lot of false positives.\n\n### Low recall, high precision:\n\n This means that we missed a lot of positive examples but those we predicted as positive are indeed positive.\n\n# F1 Score\n\nBased on the use case of the classification model, the priority is given either to precision or recall, but in some classification models, we need both of these metrics to be combined as a single one. \n\n**F1-Score is a metric that combines both precision and recall** and has an equal and relative contribution of both precision and recall.\n\nIt is the harmonic mean of precision and recall.\n\n![F1 score.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997858830/MVeG1-Ktb.png)\n\n> Remember that if your use case needs either recall or precision, one higher than the other then F1-score may not be the good metric for it. \n\n# Mean Absolute Error\n\nMean absolute error(MAE) is defined as **the average of all errors that are calculated based on values predicted by your model**.\n\n![MAE-2.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997877447/xhAb091-l.png)\n\nIt is intended to measure average model bias in a set of predictions, without considering their direction\n\n\n![MAE-1.jpg](https://cdn.hashnode.com/res/hashnode/image/upload/v1623856647925/sJPrbnngB.jpeg)\n\n# R Squared Score\n\nR squared is a measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination.\n\nIt defines the degree to which the variance in the dependent variable (or target) can be explained by the independent variable (features).\n\nFo example if the R-squared value for our predictive model is 0.8. This means that 80% of the variation in the dependent variable is explained by the independent variables.\n\nTherefore we can say that the **higher the r-squared value is, the better is the model**.\n\n![rsqared.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1623986479517/HVNZgosBn.png)\n\nSSRES: the Residual sum of squared errors\n\nSSTOT: the total sum of squared errors\n\n\n# Root Mean Squared Error\n\nRMSE is one of the most popular metrics used today for evaluating regression-based models. This is an important evaluation metric since itâ€™s essential to find the average squared error between the predicted values.\n\nRMSE **measures the average magnitude of the error**. Itâ€™s the square root of the average of squared differences between prediction and actual observation.\n\n> RMSE is highly affected by outlier values. Hence, make sure youâ€™ve removed outliers from your data set prior to using this metric.\n\nAs compared to mean absolute error, RMSE gives higher weightage and punishes large errors\n\n![rmse.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1623856974370/Mh1npqTAA.png)\n\nhere 'N' is total observations.\n\n### Correlation between MAE and RMSE\n\nBoth of these metrics express the average error of the machine learning models.\nThese two metrics can range from 0 to infinity and both of these metrics are negatively oriented scores, which means that a lower score defines better results.\n\n# ROC Curve\n\nROC curve stands for â€œReceiver Operating Characteristic\" Curve.\n\nROC curve is a graph showing the performance of a classification model at all its cut-off threshold.\n\nIn simple words, ROC Curve is the one that **tells how much your model is capable of differentiating among the different classes**.\n\nROC is a probability curve. It is a representation of the performance of your model in a graphical manner.\n\n![ROC.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1621997941907/LeLyOIDtr.png)\n\nThis curve represents:\n\n1. True positive rate (recall/sensitivity)\n2. False-positive rate (1- specificity) : FPR = FP/ FP+TN\n\nThe curve separates the space into two areas, one for good and the other for poor performance levels.\n\n# Conclusion\n\nIn this article, you have seen some important metrics that are helpful for model evaluation and are needed to be understood by a data scientist or a machine learning engineer.\n\nI hope you have found it useful. Also please feel free to suggest corrections and improvements.\n\nThanks for reading :))\n\n# Related articles you might like ðŸ˜Š\n\n- [Understanding Linear Regression](https://apoorvtyagi.tech/understanding-linear-regression) - One of the most popular algorithm in machine learning that every data scientist should know.\n\n- [Having a go at common NLP tasks using TextBlob](https://apoorvtyagi.tech/nlp-textblob) - Learn some of the basic operations to do in Natural language processing using textblob.\n\n- [Generating unique IDs in a Large scale Distributed environment](https://apoorvtyagi.tech/generating-unique-ids-in-a-large-scale-distributed-environment) - Learn how to create unique IDs for a distributed system at a large scale. Inspired from Twitter snowflake.\n\n- [Useful Resources To Learn Web Development & To Create Your Website](https://apoorvtyagi.tech/useful-resources-to-learn-web-development-and-to-create-your-website) - Resources I found helpful in my web development journey`;

module.exports = contentMD;
